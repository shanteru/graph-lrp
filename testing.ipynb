{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from components import data_handling, glrp_scipy,nn_cnn_models\n",
    "from lib import models, graph, coarsening\n",
    "\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rndm_state = 7\n",
    "np.random.seed(rndm_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GE data, X shape:  (969, 6888)\n",
      "Labels, y shape:  (969,)\n",
      "PPI network adjacency matrix, A shape:  (6888, 6888)\n",
      "X_train max 8.352902354001749\n",
      "X_train min 0.0\n",
      "X_train shape:  (872, 6888)\n",
      "X_test shape:  (97, 6888)\n",
      "y_train, shape:  (872,)\n",
      "y_test, shape:  (97,)\n",
      "Layer 0: M_0 = |V| = 10032 nodes (3144 added),|E| = 27841 edges\n",
      "Layer 1: M_1 = |V| = 5016 nodes (755 added),|E| = 24141 edges\n",
      "Layer 2: M_2 = |V| = 2508 nodes (0 added),|E| = 20997 edges\n",
      "NN architecture\n",
      "  input: M_0 = 10032\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 10032 * 32 / 2 = 160512\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 32 * 8 = 256\n",
      "    biases: F_1 = 32\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 5016 * 32 / 2 = 80256\n",
      "    weights: F_1 * F_2 * K_2 = 32 * 32 * 8 = 8192\n",
      "    biases: F_2 = 32\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 512\n",
      "    weights: M_2 * M_3 = 80256 * 512 = 41091072\n",
      "    biases: M_3 = 512\n",
      "  layer 4: fc2\n",
      "    representation: M_4 = 128\n",
      "    weights: M_3 * M_4 = 512 * 128 = 65536\n",
      "    biases: M_4 = 128\n",
      "  layer 5: logits (softmax)\n",
      "    representation: M_5 = 2\n",
      "    weights: M_4 * M_5 = 128 * 2 = 256\n",
      "    biases: M_5 = 2\n",
      "WARNING:tensorflow:From c:\\Users\\chant\\miniconda3\\envs\\lrpenv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\Users\\chant\\miniconda3\\envs\\lrpenv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_to_feature_val = \"./data/GE_PPI/GEO_HG_PPI.csv\"\n",
    "path_to_feature_graph = \"./data/GE_PPI/HPRD_PPI.csv\"\n",
    "path_to_labels = \"./data/GE_PPI/labels_GEO_HG.csv\"\n",
    "\n",
    "DP = data_handling.DataPreprocessor(path_to_feature_values=path_to_feature_val, path_to_feature_graph=path_to_feature_graph,\n",
    "                                    path_to_labels=path_to_labels)\n",
    "X = DP.get_feature_values_as_np_array()  # gene expression\n",
    "A = csr_matrix(DP.get_adj_feature_graph_as_np_array().astype(np.float32))  # adjacency matrix of the PPI network\n",
    "y = DP.get_labels_as_np_array()  # labels\n",
    "\n",
    "print(\"GE data, X shape: \", X.shape)\n",
    "print(\"Labels, y shape: \", y.shape)\n",
    "print(\"PPI network adjacency matrix, A shape: \", A.shape)\n",
    "\n",
    "X_train_unnorm, X_test_unnorm, y_train, y_test = train_test_split(X, y, test_size=0.10,\n",
    "                                                                    stratify=y, random_state=rndm_state)\n",
    "\n",
    "# Need to know which patients got into train and test subsets\n",
    "_, _, patient_indexes_train, patient_indexes_test = train_test_split(X, DP.labels.columns.values.tolist(), test_size=0.10,\n",
    "                                                                    stratify=y, random_state=rndm_state)\n",
    "\n",
    "# Data frame with test patients and corresponding ground truth labels\n",
    "patient_ind_test_df = pd.DataFrame(data={\"Patient ID\": patient_indexes_test, \"label\": y_test})\n",
    "\n",
    "# !!!\n",
    "# Making data lying in the interval [0, 8.35]\n",
    "X_train = X_train_unnorm - np.min(X)\n",
    "X_test = X_test_unnorm - np.min(X)\n",
    "\n",
    "print(\"X_train max\", np.max(X_train))\n",
    "print(\"X_train min\", np.min(X_train))\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_train, shape: \", y_train.shape)\n",
    "print(\"y_test, shape: \", y_test.shape)\n",
    "\n",
    "# coarsening the PPI graph to perform pooling in the model\n",
    "graphs, perm = coarsening.coarsen(A, levels=2, self_connections=False)\n",
    "L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "\n",
    "X_train = coarsening.perm_data(X_train, perm)\n",
    "X_test = coarsening.perm_data(X_test, perm)\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "params = dict()\n",
    "params['dir_name']       = 'GE'\n",
    "params['num_epochs']     = 100\n",
    "params['batch_size']     = 109\n",
    "params['eval_frequency'] = 40\n",
    "\n",
    "# Building blocks.\n",
    "params['filter']         = 'chebyshev5'\n",
    "params['brelu']          = 'b1relu'\n",
    "params['pool']           = 'mpool1'\n",
    "\n",
    "# Number of classes.\n",
    "C = y.max() + 1\n",
    "assert C == np.unique(y).size\n",
    "\n",
    "# Architecture.\n",
    "params['F']              = [32, 32]  # Number of graph convolutional filters.\n",
    "params['K']              = [8, 8]  # Polynomial orders.\n",
    "params['p']              = [2, 2]    # Pooling sizes.\n",
    "params['M']              = [512, 128, C]  # Output dimensionality of fully connected layers.\n",
    "\n",
    "# Optimization.\n",
    "params['regularization'] = 1e-4\n",
    "params['dropout']        = 1\n",
    "params['learning_rate']  = 1e-3\n",
    "params['decay_rate']     = 0.95\n",
    "params['momentum']       = 0\n",
    "params['decay_steps']    = n_train / params['batch_size']\n",
    "\n",
    "model = models.cgcnn(L, **params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since CGCNN cant work--> need to have another model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tCheb_conv, first layer, input shape : 10032\n",
      "\tmpool1, Pooling layer, size : 2\n",
      "\tCheb_conv layer, input shape : 5016\n",
      "\tmpool1, Pooling layer, size : 2\n",
      "\tFC layer, nodes: 512\n",
      "\tFC layer, nodes: 128\n",
      "\tLast layer, nodes: 2 \n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cheb_conv (ChebConv)        (None, 10032, 32)         288       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 5016, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " cheb_conv_1 (ChebConv)      (None, 5016, 32)          8224      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 2508, 32)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 80256)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               41091584  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,166,018\n",
      "Trainable params: 41,166,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 872 samples, validate on 97 samples\n",
      "Epoch 1/100\n",
      "872/872 [==============================] - ETA: 0s - loss: 42.9655 - acc: 0.4931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chant\\miniconda3\\envs\\lrpenv\\lib\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872/872 [==============================] - 6s 7ms/sample - loss: 42.9655 - acc: 0.4931 - val_loss: 13.1091 - val_acc: 0.5979\n",
      "Epoch 2/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 5.7718 - acc: 0.5367 - val_loss: 1.7154 - val_acc: 0.4021\n",
      "Epoch 3/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.9681 - acc: 0.5688 - val_loss: 0.7171 - val_acc: 0.6701\n",
      "Epoch 4/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.7652 - acc: 0.6365 - val_loss: 0.7610 - val_acc: 0.6186\n",
      "Epoch 5/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.6921 - acc: 0.6858 - val_loss: 0.6600 - val_acc: 0.7526\n",
      "Epoch 6/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.6522 - acc: 0.7225 - val_loss: 0.6396 - val_acc: 0.7835\n",
      "Epoch 7/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.6253 - acc: 0.7477 - val_loss: 0.6374 - val_acc: 0.7526\n",
      "Epoch 8/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.6114 - acc: 0.7718 - val_loss: 0.6315 - val_acc: 0.7526\n",
      "Epoch 9/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.5949 - acc: 0.7741 - val_loss: 0.6216 - val_acc: 0.7835\n",
      "Epoch 10/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.5749 - acc: 0.7993 - val_loss: 0.6194 - val_acc: 0.7938\n",
      "Epoch 11/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.5556 - acc: 0.7993 - val_loss: 0.6128 - val_acc: 0.7732\n",
      "Epoch 12/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.5395 - acc: 0.8177 - val_loss: 0.6176 - val_acc: 0.7526\n",
      "Epoch 13/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.5238 - acc: 0.8245 - val_loss: 0.6121 - val_acc: 0.7629\n",
      "Epoch 14/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.5078 - acc: 0.8142 - val_loss: 0.6910 - val_acc: 0.7320\n",
      "Epoch 15/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.5271 - acc: 0.8142 - val_loss: 0.6006 - val_acc: 0.7835\n",
      "Epoch 16/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.4934 - acc: 0.8211 - val_loss: 0.6629 - val_acc: 0.6804\n",
      "Epoch 17/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.4816 - acc: 0.8314 - val_loss: 0.5973 - val_acc: 0.8041\n",
      "Epoch 18/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.4495 - acc: 0.8612 - val_loss: 0.5997 - val_acc: 0.7835\n",
      "Epoch 19/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.4255 - acc: 0.8589 - val_loss: 0.5993 - val_acc: 0.7938\n",
      "Epoch 20/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.4127 - acc: 0.8681 - val_loss: 0.5937 - val_acc: 0.7732\n",
      "Epoch 21/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.4089 - acc: 0.8727 - val_loss: 0.5965 - val_acc: 0.7732\n",
      "Epoch 22/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.4051 - acc: 0.8819 - val_loss: 0.6361 - val_acc: 0.7526\n",
      "Epoch 23/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.3939 - acc: 0.8842 - val_loss: 0.6193 - val_acc: 0.7629\n",
      "Epoch 24/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.3727 - acc: 0.8888 - val_loss: 0.5953 - val_acc: 0.7835\n",
      "Epoch 25/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.3631 - acc: 0.8956 - val_loss: 0.5973 - val_acc: 0.7835\n",
      "Epoch 26/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.3495 - acc: 0.9071 - val_loss: 0.6004 - val_acc: 0.7732\n",
      "Epoch 27/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.3422 - acc: 0.9197 - val_loss: 0.5944 - val_acc: 0.8041\n",
      "Epoch 28/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.3319 - acc: 0.9255 - val_loss: 0.5963 - val_acc: 0.7835\n",
      "Epoch 29/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.3233 - acc: 0.9312 - val_loss: 0.5995 - val_acc: 0.7835\n",
      "Epoch 30/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.3250 - acc: 0.9220 - val_loss: 0.6002 - val_acc: 0.7938\n",
      "Epoch 31/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.3050 - acc: 0.9381 - val_loss: 0.6153 - val_acc: 0.7732\n",
      "Epoch 32/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.3039 - acc: 0.9346 - val_loss: 0.6003 - val_acc: 0.7938\n",
      "Epoch 33/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.3069 - acc: 0.9300 - val_loss: 0.6448 - val_acc: 0.7423\n",
      "Epoch 34/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.3030 - acc: 0.9312 - val_loss: 0.5972 - val_acc: 0.7938\n",
      "Epoch 35/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2799 - acc: 0.9541 - val_loss: 0.6100 - val_acc: 0.7835\n",
      "Epoch 36/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2769 - acc: 0.9564 - val_loss: 0.6363 - val_acc: 0.7732\n",
      "Epoch 37/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2772 - acc: 0.9461 - val_loss: 0.6683 - val_acc: 0.7732\n",
      "Epoch 38/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2763 - acc: 0.9415 - val_loss: 0.6547 - val_acc: 0.7732\n",
      "Epoch 39/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2712 - acc: 0.9564 - val_loss: 0.6414 - val_acc: 0.7835\n",
      "Epoch 40/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2614 - acc: 0.9633 - val_loss: 0.6020 - val_acc: 0.8041\n",
      "Epoch 41/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2529 - acc: 0.9690 - val_loss: 0.6069 - val_acc: 0.7835\n",
      "Epoch 42/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2483 - acc: 0.9736 - val_loss: 0.6117 - val_acc: 0.7835\n",
      "Epoch 43/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2449 - acc: 0.9690 - val_loss: 0.6067 - val_acc: 0.8144\n",
      "Epoch 44/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2440 - acc: 0.9702 - val_loss: 0.6069 - val_acc: 0.8144\n",
      "Epoch 45/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2417 - acc: 0.9690 - val_loss: 0.6402 - val_acc: 0.7938\n",
      "Epoch 46/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2448 - acc: 0.9610 - val_loss: 0.6532 - val_acc: 0.7835\n",
      "Epoch 47/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2387 - acc: 0.9713 - val_loss: 0.6140 - val_acc: 0.7938\n",
      "Epoch 48/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2308 - acc: 0.9782 - val_loss: 0.6092 - val_acc: 0.8144\n",
      "Epoch 49/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2309 - acc: 0.9771 - val_loss: 0.6360 - val_acc: 0.7938\n",
      "Epoch 50/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.2259 - acc: 0.9794 - val_loss: 0.6069 - val_acc: 0.8144\n",
      "Epoch 51/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.2250 - acc: 0.9828 - val_loss: 0.6153 - val_acc: 0.7938\n",
      "Epoch 52/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.2213 - acc: 0.9817 - val_loss: 0.6108 - val_acc: 0.8041\n",
      "Epoch 53/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.2197 - acc: 0.9839 - val_loss: 0.6321 - val_acc: 0.8041\n",
      "Epoch 54/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.2197 - acc: 0.9805 - val_loss: 0.6147 - val_acc: 0.7938\n",
      "Epoch 55/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.2230 - acc: 0.9805 - val_loss: 0.6107 - val_acc: 0.8144\n",
      "Epoch 56/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.2146 - acc: 0.9805 - val_loss: 0.6485 - val_acc: 0.7938\n",
      "Epoch 57/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.2167 - acc: 0.9805 - val_loss: 0.6166 - val_acc: 0.7938\n",
      "Epoch 58/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.2112 - acc: 0.9862 - val_loss: 0.6204 - val_acc: 0.7938\n",
      "Epoch 59/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2100 - acc: 0.9862 - val_loss: 0.6229 - val_acc: 0.7938\n",
      "Epoch 60/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2089 - acc: 0.9851 - val_loss: 0.6259 - val_acc: 0.7938\n",
      "Epoch 61/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2080 - acc: 0.9874 - val_loss: 0.6194 - val_acc: 0.7938\n",
      "Epoch 62/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2064 - acc: 0.9885 - val_loss: 0.6206 - val_acc: 0.7938\n",
      "Epoch 63/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2061 - acc: 0.9862 - val_loss: 0.6325 - val_acc: 0.7938\n",
      "Epoch 64/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2074 - acc: 0.9874 - val_loss: 0.6153 - val_acc: 0.8041\n",
      "Epoch 65/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2064 - acc: 0.9851 - val_loss: 0.6365 - val_acc: 0.8041\n",
      "Epoch 66/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2016 - acc: 0.9874 - val_loss: 0.6173 - val_acc: 0.8041\n",
      "Epoch 67/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.2033 - acc: 0.9839 - val_loss: 0.6426 - val_acc: 0.8041\n",
      "Epoch 68/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.2037 - acc: 0.9885 - val_loss: 0.6197 - val_acc: 0.8041\n",
      "Epoch 69/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.2007 - acc: 0.9885 - val_loss: 0.6319 - val_acc: 0.7938\n",
      "Epoch 70/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.1996 - acc: 0.9897 - val_loss: 0.6223 - val_acc: 0.7938\n",
      "Epoch 71/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1992 - acc: 0.9897 - val_loss: 0.6273 - val_acc: 0.7938\n",
      "Epoch 72/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.1990 - acc: 0.9897 - val_loss: 0.6216 - val_acc: 0.8041\n",
      "Epoch 73/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.1978 - acc: 0.9908 - val_loss: 0.6345 - val_acc: 0.7938\n",
      "Epoch 74/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.1979 - acc: 0.9874 - val_loss: 0.6225 - val_acc: 0.8041\n",
      "Epoch 75/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.1970 - acc: 0.9897 - val_loss: 0.6342 - val_acc: 0.7938\n",
      "Epoch 76/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1951 - acc: 0.9897 - val_loss: 0.6244 - val_acc: 0.7938\n",
      "Epoch 77/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1953 - acc: 0.9908 - val_loss: 0.6274 - val_acc: 0.7938\n",
      "Epoch 78/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1944 - acc: 0.9897 - val_loss: 0.6294 - val_acc: 0.7938\n",
      "Epoch 79/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1939 - acc: 0.9897 - val_loss: 0.6291 - val_acc: 0.7938\n",
      "Epoch 80/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1937 - acc: 0.9897 - val_loss: 0.6283 - val_acc: 0.7938\n",
      "Epoch 81/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1931 - acc: 0.9897 - val_loss: 0.6320 - val_acc: 0.7938\n",
      "Epoch 82/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1931 - acc: 0.9908 - val_loss: 0.6314 - val_acc: 0.7938\n",
      "Epoch 83/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1924 - acc: 0.9908 - val_loss: 0.6277 - val_acc: 0.7938\n",
      "Epoch 84/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1920 - acc: 0.9908 - val_loss: 0.6335 - val_acc: 0.7938\n",
      "Epoch 85/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1920 - acc: 0.9920 - val_loss: 0.6324 - val_acc: 0.7938\n",
      "Epoch 86/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1922 - acc: 0.9908 - val_loss: 0.6276 - val_acc: 0.7938\n",
      "Epoch 87/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1911 - acc: 0.9908 - val_loss: 0.6360 - val_acc: 0.7938\n",
      "Epoch 88/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.1909 - acc: 0.9920 - val_loss: 0.6300 - val_acc: 0.7938\n",
      "Epoch 89/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.1907 - acc: 0.9908 - val_loss: 0.6287 - val_acc: 0.7938\n",
      "Epoch 90/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.1899 - acc: 0.9920 - val_loss: 0.6347 - val_acc: 0.7938\n",
      "Epoch 91/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.1901 - acc: 0.9920 - val_loss: 0.6331 - val_acc: 0.7938\n",
      "Epoch 92/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.1897 - acc: 0.9920 - val_loss: 0.6338 - val_acc: 0.7938\n",
      "Epoch 93/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1894 - acc: 0.9920 - val_loss: 0.6323 - val_acc: 0.7938\n",
      "Epoch 94/100\n",
      "872/872 [==============================] - 5s 5ms/sample - loss: 0.1893 - acc: 0.9908 - val_loss: 0.6304 - val_acc: 0.7938\n",
      "Epoch 95/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1893 - acc: 0.9931 - val_loss: 0.6360 - val_acc: 0.7938\n",
      "Epoch 96/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1890 - acc: 0.9931 - val_loss: 0.6312 - val_acc: 0.7938\n",
      "Epoch 97/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1886 - acc: 0.9920 - val_loss: 0.6332 - val_acc: 0.7938\n",
      "Epoch 98/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1884 - acc: 0.9931 - val_loss: 0.6336 - val_acc: 0.7938\n",
      "Epoch 99/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1884 - acc: 0.9920 - val_loss: 0.6355 - val_acc: 0.7938\n",
      "Epoch 100/100\n",
      "872/872 [==============================] - 5s 6ms/sample - loss: 0.1881 - acc: 0.9920 - val_loss: 0.6327 - val_acc: 0.7938\n",
      "\n",
      "\tTraining time: 480.2193319797516 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chant\\miniconda3\\envs\\lrpenv\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Accuraccy: 0.7938, test F1: 0.7952\n"
     ]
    }
   ],
   "source": [
    "# Additional parameters needed\n",
    "params[\"L\"] = L\n",
    "\n",
    "model = nn_cnn_models.get_cheb_net_model(**params)\n",
    "my_cheb_net_for_cv = nn_cnn_models.MyChebNet(params, model)\n",
    "my_cheb_net_for_cv.create(feature_number=None)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "start = time.time()\n",
    "my_cheb_net_for_cv.fit(x=np.expand_dims(X_train, 2), y=y_train, validation_data=[np.expand_dims(X_test, 2), y_test],\n",
    "                        verbose=1)\n",
    "end = time.time()\n",
    "print(\"\\n\\tTraining time:\", end-start, \"\\n\")\n",
    "# y_preds = my_cheb_net_for_cv.predict(X_test)\n",
    "\n",
    "y_preds = my_cheb_net_for_cv.predict(np.expand_dims(X_test, 2))\n",
    "acc = accuracy_score(y_test, np.argmax(y_preds, axis=1))\n",
    "f1 = f1_score(y_test, np.argmax(y_preds, axis=1), average='weighted')\n",
    "print(\"test Accuraccy: %0.4f, test F1: %0.4f\" % (acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest F1 weighted:  0.7951614641330508\n",
      "\tTest Accuraccy: 0.7938144329896907 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_preds = my_cheb_net_for_cv.predict(np.expand_dims(X_test, 2))\n",
    "acc = accuracy_score(y_test, np.argmax(y_preds, axis=1))\n",
    "f1 = f1_score(y_test, np.argmax(y_preds, axis=1), average='weighted')\n",
    "print(\"\\tTest F1 weighted: \", f1)\n",
    "print(\"\\tTest Accuraccy:\", acc, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTest AUC: 0.8470380194518126\n"
     ]
    }
   ],
   "source": [
    "probas_ = model.predict(np.expand_dims(X_test, 2))\n",
    "labels_by_network = np.argmax(probas_, axis=1)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, probas_[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"\\n\\tTest AUC:\", roc_auc) # np.argmax(y_preds, axis=2)[:, 0] fot categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip this this is for CGCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'get_probabilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m probas_ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mget_probabilities(X_test)\n\u001b[0;32m      2\u001b[0m labels_by_network \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(probas_, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m fpr, tpr, _ \u001b[39m=\u001b[39m roc_curve(y_test, probas_[:, \u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'get_probabilities'"
     ]
    }
   ],
   "source": [
    "probas_ = model.get_probabilities(X_test)\n",
    "labels_by_network = np.argmax(probas_, axis=1)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, probas_[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "f1 = 100 * f1_score(y_test, labels_by_network, average='weighted')\n",
    "acc = accuracy_score(y_test, labels_by_network)\n",
    "print(\"\\n\\tTest AUC:\", roc_auc) # np.argmax(y_preds, axis=2)[:, 0] fot categorical\n",
    "print(\"\\tTest F1 weighted: \", f1)\n",
    "print(\"\\tTest Accuraccy:\", acc, \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m I \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39meye(C)\n\u001b[0;32m      4\u001b[0m tmp \u001b[39m=\u001b[39m I[labels_by_network]\n\u001b[1;32m----> 5\u001b[0m labels_hot_encoded \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((model\u001b[39m.\u001b[39;49mbatch_size, C))\n\u001b[0;32m      6\u001b[0m labels_hot_encoded[\u001b[39m0\u001b[39m:tmp\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m0\u001b[39m:tmp\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m tmp\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabels_hot_encoded.shape\u001b[39m\u001b[39m\"\u001b[39m, labels_hot_encoded\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'batch_size'"
     ]
    }
   ],
   "source": [
    "# !!!\n",
    "# Creating hot-encoded labels for GLRP\n",
    "I = np.eye(C)\n",
    "tmp = I[labels_by_network]\n",
    "labels_hot_encoded = np.ones((model.batch_size, C))\n",
    "labels_hot_encoded[0:tmp.shape[0], 0:tmp.shape[1]] = tmp\n",
    "print(\"labels_hot_encoded.shape\", labels_hot_encoded.shape)\n",
    "\n",
    "dir_to_save = \"./results/\"\n",
    "\n",
    "print(\"labels_by_network type\", labels_by_network.dtype)\n",
    "print(\"y_test type\", y_test.dtype)\n",
    "concordance = y_test == labels_by_network\n",
    "print(y_test)\n",
    "print(labels_by_network)\n",
    "print(concordance)\n",
    "concordance = concordance.astype(int)\n",
    "out_labels_conc_df = pd.DataFrame(np.array([labels_by_network, concordance]).transpose(),\n",
    "                                    columns=[\"Predicted\", \"Concordance\"])\n",
    "concordance_df = patient_ind_test_df.join(out_labels_conc_df)\n",
    "concordance_df.to_csv(path_or_buf = dir_to_save + \"predicted_concordance_lrp0.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tCalculating Polynomials of Laplace Matrices... Time:  18.933253288269043 \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\chant\\OneDrive\\Desktop\\XAI\\graph-lrp\\lib\\..\\checkpoints\\GE\\model-800\n",
      "\n",
      "    Relevance calculation:\n",
      "\tFully connected: logits\n",
      "\tFully connected: fc2\n",
      "\tFully connected: fc1\n",
      "\tFlatten layer: flatten\n",
      "\tPooling: conv2 pooling\n",
      "\t\tname of pooling: mpool1\n",
      "\tConvolution:  conv2 \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\chant\\OneDrive\\Desktop\\XAI\\graph-lrp\\lib\\..\\checkpoints\\GE\\model-800\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\chant\\OneDrive\\Desktop\\XAI\\graph-lrp\\lib\\..\\checkpoints\\GE\\model-800\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\chant\\OneDrive\\Desktop\\XAI\\graph-lrp\\lib\\..\\checkpoints\\GE\\model-800\n",
      "\n",
      "\tconv2, relevance propagation time is:  341.6468632221222\n",
      "\tPooling: conv1 pooling\n",
      "\t\tname of pooling: mpool1\n",
      "\tConvolution, the first layer: conv1 \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\chant\\OneDrive\\Desktop\\XAI\\graph-lrp\\lib\\..\\checkpoints\\GE\\model-800\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\chant\\OneDrive\\Desktop\\XAI\\graph-lrp\\lib\\..\\checkpoints\\GE\\model-800\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\chant\\OneDrive\\Desktop\\XAI\\graph-lrp\\lib\\..\\checkpoints\\GE\\model-800\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\chant\\OneDrive\\Desktop\\XAI\\graph-lrp\\lib\\..\\checkpoints\\GE\\model-800\n",
      "\n",
      "\tconv1, relevance propagation time is:  63.8696870803833 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # !!!\n",
    "    # CALCULATION OF RELEVANCES\n",
    "    # CAN TAKE QUITE SOME TIME (UP to 10 MIN, Intel(R) Xeon(R) CPU E5-1620 v2 @ 3.70GHz, 32 GB RAM)\n",
    "from components import glrp_0_scipy\n",
    "glrp = glrp_scipy.GraphLayerwiseRelevancePropagation(model, X_test, labels_hot_encoded)\n",
    "rel = glrp.get_relevances()[-1][:X_test.shape[0], :]\n",
    "rel = coarsening.perm_data_back(rel, perm, X.shape[1])\n",
    "rel_df = pd.DataFrame(rel, columns=DP.feature_names)\n",
    "rel_df = pd.DataFrame(data={\"Patient ID\": patient_indexes_test}).join(rel_df)\n",
    "rel_df.to_csv(path_or_buf = dir_to_save + \"relevances_rendered_class_lrp.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chant\\miniconda3\\envs\\lrpenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import quantus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 10032)\n",
      "Warnings and information:\n",
      " (1) The Faithfulness Correlation metric is likely to be sensitive to the choice of baseline value 'perturb_baseline', size of subset |S| 'subset_size' and the number of runs (for each input and explanation pair) 'nr_runs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Bhatt, Umang, Adrian Weller, and José MF Moura. 'Evaluating and aggregating feature-based model explanations.' arXiv preprint arXiv:2005.00631 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'cgcnn' object has no attribute 'shape_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m explanations \u001b[39m=\u001b[39m rel_df\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(X_test\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> 3\u001b[0m quantus\u001b[39m.\u001b[39;49mFaithfulnessCorrelation()\u001b[39m.\u001b[39;49mevaluate_instance(model,X_test,y_test,explanations,labels_by_network)\n",
      "File \u001b[1;32mc:\\Users\\chant\\miniconda3\\envs\\lrpenv\\lib\\site-packages\\quantus\\metrics\\faithfulness\\faithfulness_correlation.py:295\u001b[0m, in \u001b[0;36mFaithfulnessCorrelation.evaluate_instance\u001b[1;34m(self, model, x, y, a, s)\u001b[0m\n\u001b[0;32m    292\u001b[0m a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m    294\u001b[0m \u001b[39m# Predict on input.\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m x_input \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mshape_input(x, x\u001b[39m.\u001b[39mshape, channel_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    296\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(model\u001b[39m.\u001b[39mpredict(x_input)[:, y])\n\u001b[0;32m    298\u001b[0m pred_deltas \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'cgcnn' object has no attribute 'shape_input'"
     ]
    }
   ],
   "source": [
    "explanations = rel_df.to_numpy()\n",
    "print(X_test.shape)\n",
    "quantus.FaithfulnessCorrelation().evaluate_instance(model,X_test,y_test,explanations,labels_by_network)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4093d01f7f921b36750c71dd400cd1ca82a97dfefa3b5a2a792653e30b2d1fe0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
